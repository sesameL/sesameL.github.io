{"posts":[{"title":"leetcode-algorithm","text":"DP（dynamic programming） 剑指 Offer 63. 股票的最大利润题目 1234567891011class Solution:def maxProfit(self, prices: List[int]) -&gt; int: max = 0 min = float(inf) for i in prices: min = min(i, min) max = (max, i - min) if max &lt; 0: return 0 return max 状态定义： 设动态规划列表 dpdpdp ，dp[i]dp[i]dp[i] 代表以 prices[i]prices[i]prices[i] 为结尾的子数组的最大利润 状态转移方程：买卖股票一次，所以比较前i-1天的最大profit和第i天的profit note: 比较可以用min()，max()代替if语句","link":"/2022/09/24/leetcode-algorithm/"},{"title":"welcome to my blog","text":"This is my first post. Check my GitHub for more info. If you get any problems you can ask me via my Email:se5ame@foxmail.com. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2022/09/21/hello-world/"},{"title":"pandas &amp; plot","text":"plotsns.pairplot看数据相关性，两两比较 1234from sklearn.datasets import load_irisimport matplotlib.pyplot as pltimport seaborn as snsimport pandas as pd parameter data为画图使用的数据，hue 是数据的label 1sns.pairplot(data,hue='target') vars:只留几个特征两两比较 12sns.pairplot(data1,hue='target',vars=['sepal length (cm)', 'sepal width (cm)']); palette 调整颜色 123sns.pairplot(data1,hue='target',palette={'setosa':'b', 'versicolor':'deepskyblue', 'virginica':'#43C6C3'}); sns.heatmap（热力图）","link":"/2022/09/27/pandas/"},{"title":"statistics","text":"概率论1.极大似然估计MLE利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值前提：满足独立同分布 对于这个函数：p(x \\mid \\theta) 其中 $ x $ 表示 具体数据 $ \\theta $ 表示模型参数如果 $ \\theta $ 是已知确定的，x是变量，这个函数叫做 概率函数(probability function)，它描述对于不同的样本点 ，其出现概率是多少。如果 $ x $ 是已知确定的，\\theta 是变量，这个函数叫做 似然函数(likelihood function) , 它描述对于不同的模型参数，出现 这个样本点的概率是多少。 e.g.1.有一个罐子，里面有黑白两种颜色的球，数目多少不知，两种颜色的比例也不知。想知道罐中白球和黑球的比例前面的一百次重复记录中，有七十次是白球，请问罐中白球所占的比例最有可能是多少? 解：如果第一次抽象的结果记为x1,第二次抽样的结果记为x2….那么样本结果为(x1,x2…..,x100)。这样，我们可以得到如下表达式：P(样本结果|Model) \\begin{aligned} &=P(x 1, x 2, \\ldots, x 100 \\mid \\text { Model }) \\\\ &=P(x 1 \\mid M e l) P(x 2 \\mid M) \\ldots P(x 100 \\mid M) \\\\ &=p^{\\wedge} 70(1-p)^{\\wedge} 30 . \\end{aligned}样本结果出现的可能性最大，也就是使得 p^70(1-p)^30 值最大，那么我们就可以看成是p的方程，求导即可！ 那么既然事情已经发生了，为什么不让这个出现的结果的可能性最大呢？这也就是最大似然估计的核心。通常将上式两边去对数，将乘法变为加法，结果不变，方便求导 2.贝叶斯和最大后验MAP 2.1贝叶斯公式 \\mathrm{P}(\\mathrm{A} \\mid \\mathrm{B})=\\frac{\\mathrm{P}(\\mathrm{B} \\mid \\mathrm{A}) \\mathrm{P}(\\mathrm{A})}{\\mathrm{P}(\\mathrm{B})} 将 $P(B)$ 用全概率公式展开（连续型变量用积分表离散型用求和表示） \\mathrm{P}(\\mathrm{A} \\mid \\mathrm{B})=\\frac{\\mathrm{P}(\\mathrm{B} \\mid \\mathrm{A}) \\mathrm{P}(\\mathrm{A})}{\\mathrm{P}(\\mathrm{B} \\mid \\mathrm{A}) \\mathrm{P}(\\mathrm{A})+\\mathrm{P}(\\mathrm{B} \\mid \\sim \\mathrm{A}) \\mathrm{P}(\\sim \\mathrm{A})} 其中 $ P(A) $ 为先验，为已知值。对于投硬币的例子来看，我们认为（”先验地知道“）$ \\theta $ 取0.5的概率很大，取其他值的概率小一些。可以用一个高斯分布来具体描述这个先验知识，例如假设 $P(\\theta)$ 为均值0.5，方差0.1的高斯函数，$ P(B \\mid A)$ 称似然函数$P(A \\mid B)$ 称为后验 最大后验估计MAP 其中分母为已知值（这是一个可以由数据集得到的值），$P(\\theta)$ 也已知，MAP于MLE的区别就是MAP多乘了一个先验 $P(\\theta)$ 朴素贝叶斯 给定训练数据集，其中每个样本x都包括n维特征 $x=\\left(x_1, x_2, \\cdots, x_n\\right)$，类标记集合含有k种类别，即 $y=\\left(y_1, y_2, \\cdots, y_n\\right)$ 此时的后验记为 P\\left(y_k \\mid x\\right)=\\frac{P\\left(x \\mid y_k\\right) \\times P\\left(y_k\\right)}{\\sum_k P\\left(x \\mid y_k\\right) \\times P\\left(y_k\\right)} 朴素贝叶斯算法对条件概率分布作出了独立性的假设，通俗地讲就是说假设各个维度的特征 $ {x_1},{x_2}, \\cdots ,{x_n} $ 互相独立， 所以分子分母的条件概率都能改写为 P\\left(x \\mid y_k\\right)=P\\left(x_1, x_2, \\cdots, x_n \\mid y_k\\right)=\\prod_{i=1}^n P\\left(x_i \\mid y_k\\right) 所以朴素贝叶斯可以改写为： f(x)=\\underset{y_k}{\\arg \\max } P\\left(y_k \\mid x\\right)=\\underset{y_k}{\\arg \\max } \\frac{P\\left(y_k\\right) \\times \\prod_{i=1}^n P\\left(x_i \\mid y_k\\right)}{\\sum_k P\\left(y_k\\right) \\times \\prod_{i=1}^n P\\left(x_i \\mid y_k\\right)} 因为对所有的 $ y_k $ ，上式中的分母的值都是一样的，所以可以忽略分母部分 优点 朴素贝叶斯模型有稳定的分类效率。 对小规模的数据表现很好，能处理多分类任务，适合增量式训练，尤其是数据量超出内存时，可以一批批的去增量训练。 对缺失数据不太敏感，算法也比较简单，常用于文本分类。 缺点 需要知道先验概率，且先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。 对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）。 蒙特卡洛 理论基础是大数定律。大数定律是描述相当多次数重复试验的结果的定律，在大数定理的保证下：利用事件发生的 频率 作为事件发生的 概率的近似值。 蒙特卡罗方法一般分为三个步骤，包括构造随机的概率的过程，从构造随机概率分布中抽样，求解估计量。 一般情况下，蒙特卡罗算法的特点是，采样越多，越近似最优解，而永远不是最优解。 优点：对于具有统计性质的问题可以直接进行解决，对于连续性的问题也不必进行离散化处理。 缺点：1、对于确定性问题转化成随机性问题做的估值处理，丧失精确性，得到一个接近准确的N值也不太容易。 2、如果解空间的可能情况很多则很难求解（NP问题）","link":"/2022/09/21/statistics/"}],"tags":[{"name":"leetcode","slug":"leetcode","link":"/tags/leetcode/"},{"name":"algorithm","slug":"algorithm","link":"/tags/algorithm/"},{"name":"ML","slug":"ML","link":"/tags/ML/"}],"categories":[],"pages":[]}