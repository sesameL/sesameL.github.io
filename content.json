{"posts":[{"title":"feature-engineering","text":"01 normalization 不做归一化的结果会倾向于数值差别比较大的特征 Q1 为什么需要对数值类型的特征做归一化? 将所有的特征都统一到一个大致相同的数值区间内。 包括线性函数归一化(Min-Max Scaling)，使 结果映射到[0, 1]的范围，实现对原始数据的等比缩放,() X_{\\text {norm }}=\\frac{X-X_{\\min }}{X_{\\max }-X_{\\min }} 标准化：零均值标准化(Z-Score Normalization) 它会将原始数据映射到均值为 0、标准差为1的分布上。具体来说，假设原始特征的均值为μ、标准差为σ，那么 归一化公式定义为 z=\\frac{x-\\mu}{\\sigma} 归一化的优点：e.g. 容易更快地通过梯度下降找到最优解。 数据归一化对需要使用梯度下降的模型起作用，对决策树模型不产生影响 两者比较： 计算距离中发挥相同的作用，应该选择标准化，标准化更适合现代嘈杂大数据场景 想保留原始数据中由标准差所反映的潜在权重关系，或数据不符合正态分布时，选择归一化。 RE Q2.逻辑回归必须要进行标准化吗？如果你不用正则，标准化并不是必须的，如果用正则，那么标准化是必须的。当然，无论哪种情况，标准化都有益无害，不用正则时，归一化是影响参数更新的快慢，但加上正则，则正则项偏向于关注数值范围大参数。 Q3.如何处理类别型特征有限选项内取值的特征，e.g. gender（male, female），blood types（AB， A，B）通常为字符串，通常要转为数值型（除决策树等少数model） 序号编码：有大小关系的， e.g. 高度，成绩，可直接转为1，2，3（保留了大小关系） one-hot ： 不具有大小关系的， 优点： 使用稀疏向量来节省空间（某一维取值为 1，其他位置取值均为0） 将离散特征扩展到欧式空间，离散特征就对应对应空间中的某个点 便于配合特征选择降维 二进制编码 且维数少于独热编码，节省了存储空间。 特征组合 把一阶离散特征两两组合，构成高阶组合特征。若组合后的矩阵 M x N 太大了，则进行矩阵分解将需要学习的参数的规模变为m×k+n×k Q3.特征组合寻找方法见决策树， Q4.文本模型 模型 词袋模型：就是将每篇文章看成一袋子 词，并忽略每个词出现的顺序。具体地说，就是将整段文本以词为单位切分开， 然后每篇文章可以表示成一个长向量，向量中的每一维代表一个单词，而该维对 应的权重则反映了这个词在原文章中的重要程度。常用TF-IDF来计算权重， TF-IDF \\operatorname{TF}-\\operatorname{IDF}(t, d)=\\mathrm{TF}(t, d) \\times \\operatorname{IDF}(t),其中TF(t,d)为单词t在文档d中出现的频率，IDF(t)是逆文档频率，用来衡量单词t对表达语义所起的重要性，表示为直观的解释是，如果一个单词在非常多的文章里面都出现，那么它可能是一个比较通用的词汇，对于区分某篇文章特殊语义的贡献较小，因此对权重做一定惩罚 n-gram可以将连续 出现的n个词(n≤N)组成的词组(N-gram)也作为一个单独的特征放到向量表示 中去，构成N-gram模型。同时还会对单词进行词干抽取(Word Stemming)处理，即将不 同词性的单词统一成为同一词干的形式。 Word2Vec一种浅层的神经网络模型，由CBOW(Continues Bag of Words)和Skip-gram组合。CBOW的目标是根据上下文出现的词语来预测当前词的生成概率;而Skip-gram是根据当前词来预测上下文中各词的生成概率。 LDALDA是利用文档中单词的共现关 系来对单词按主题聚类，也可以理解为对“文档-单词”矩阵进行分解，得到“文档- 主题”和“主题-单词”两个概率分布。而Word2Vec其实是对“上下文-单词”矩阵进行 学习，其中上下文由周围的几个单词组成，由此得到的词向量表示更多地融入了 上下文共现的特征。 图数据不足的处理方法清洗特征处理 时间类 间隔型：离某一特殊时间点的距离 离散型：是否属于某一特殊时间段 组合型 文本型 n-gram TF-IDF 预处理- scaling（便于收敛）","link":"/2022/09/28/feature-engineering/"},{"title":"welcome to my blog","text":"This is my first post. Check my GitHub for more info. If you get any problems you can ask me via my Email:se5ame@foxmail.com. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2022/09/21/hello-world/"},{"title":"leetcode-algorithm","text":"DP（dynamic programming） note:从较小规模的问题通过递归的方式来解决问题，通过定义：初始化状态和 状态转移方程 来求解 剑指 Offer 63. 股票的最大利润题目 1234567891011class Solution:def maxProfit(self, prices: List[int]) -&gt; int: max = 0 min = float(inf) for i in prices: min = min(i, min) max = (max, i - min) if max &lt; 0: return 0 return max 状态定义： 设动态规划列表 dpdpdp ，dp[i]dp[i]dp[i] 代表以 prices[i]prices[i]prices[i] 为结尾的子数组的最大利润 状态转移方程：买卖股票一次，所以比较前i-1天的最大profit和第i天的profit note: 比较可以用min()，max()代替if语句 斐波那契 和 青蛙跳台阶","link":"/2022/09/24/leetcode-algorithm/"},{"title":"model-estimation","text":"模型评估Accuracy的局限性 当样本不平衡时acc不能作为评估标准 精确率与召回率的权衡精确率Precision：‘分类正确的正样本个数占分类器判定为正样本的样本’个数的比例。召回率recall：’分类正确的正样本个数占真正的正样本个数’的比例。","link":"/2022/10/04/model-estimation/"},{"title":"pandas &amp; plot","text":"plot（visualization）sns.pairplot看数据相关性，两两比较 1234from sklearn.datasets import load_irisimport matplotlib.pyplot as pltimport seaborn as snsimport pandas as pd parameter data为画图使用的数据，hue 是数据的label 1sns.pairplot(data,hue='target') vars:只留几个特征两两比较 12sns.pairplot(data1,hue='target',vars=['sepal length (cm)', 'sepal width (cm)']); palette 调整颜色 123sns.pairplot(data1,hue='target',palette={'setosa':'b', 'versicolor':'deepskyblue', 'virginica':'#43C6C3'}); sns.heatmap（热力图）123456789101112# 使用seaborn画图而不是pltsns.set()# data:数据 square:是否是正方形 vmax:最大值 vmin:最小值 robust:排除极端值影响sns.heatmap(data=grid, square=True, vmax=20, vmin=0, robust=True)# 标题plt.title(&quot;test&quot;)# 保存图片plt.savefig(&quot;../data/grid.png&quot;)# 显示图片plt.show() 如果是DataFrame则以列名标记，而不是index","link":"/2022/09/27/pandas/"},{"title":"statistics","text":"概率论1.极大似然估计MLE利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值前提：满足独立同分布 对于这个函数：p(x \\mid \\theta) 其中 $ x $ 表示 具体数据 $ \\theta $ 表示模型参数如果 $ \\theta $ 是已知确定的，x是变量，这个函数叫做 概率函数(probability function)，它描述对于不同的样本点 ，其出现概率是多少。如果 $ x $ 是已知确定的，\\theta 是变量，这个函数叫做 似然函数(likelihood function) , 它描述对于不同的模型参数，出现 这个样本点的概率是多少。 e.g.1.有一个罐子，里面有黑白两种颜色的球，数目多少不知，两种颜色的比例也不知。想知道罐中白球和黑球的比例前面的一百次重复记录中，有七十次是白球，请问罐中白球所占的比例最有可能是多少? 解：如果第一次抽象的结果记为x1,第二次抽样的结果记为x2….那么样本结果为(x1,x2…..,x100)。这样，我们可以得到如下表达式：P(样本结果|Model) \\begin{aligned} &=P(x 1, x 2, \\ldots, x 100 \\mid \\text { Model }) \\\\ &=P(x 1 \\mid M e l) P(x 2 \\mid M) \\ldots P(x 100 \\mid M) \\\\ &=p^{\\wedge} 70(1-p)^{\\wedge} 30 . \\end{aligned}样本结果出现的可能性最大，也就是使得 p^70(1-p)^30 值最大，那么我们就可以看成是p的方程，求导即可！ 那么既然事情已经发生了，为什么不让这个出现的结果的可能性最大呢？这也就是最大似然估计的核心。通常将上式两边去对数，将乘法变为加法，结果不变，方便求导 2.贝叶斯和最大后验MAP 2.1贝叶斯公式 \\mathrm{P}(\\mathrm{A} \\mid \\mathrm{B})=\\frac{\\mathrm{P}(\\mathrm{B} \\mid \\mathrm{A}) \\mathrm{P}(\\mathrm{A})}{\\mathrm{P}(\\mathrm{B})} 将 $P(B)$ 用全概率公式展开（连续型变量用积分表离散型用求和表示） \\mathrm{P}(\\mathrm{A} \\mid \\mathrm{B})=\\frac{\\mathrm{P}(\\mathrm{B} \\mid \\mathrm{A}) \\mathrm{P}(\\mathrm{A})}{\\mathrm{P}(\\mathrm{B} \\mid \\mathrm{A}) \\mathrm{P}(\\mathrm{A})+\\mathrm{P}(\\mathrm{B} \\mid \\sim \\mathrm{A}) \\mathrm{P}(\\sim \\mathrm{A})} 其中 $ P(A) $ 为先验，为已知值。对于投硬币的例子来看，我们认为（”先验地知道“）$ \\theta $ 取0.5的概率很大，取其他值的概率小一些。可以用一个高斯分布来具体描述这个先验知识，例如假设 $P(\\theta)$ 为均值0.5，方差0.1的高斯函数，$ P(B \\mid A)$ 称似然函数$P(A \\mid B)$ 称为后验 最大后验估计MAP 其中分母为已知值（这是一个可以由数据集得到的值），$P(\\theta)$ 也已知，MAP于MLE的区别就是MAP多乘了一个先验 $P(\\theta)$ 朴素贝叶斯 给定训练数据集，其中每个样本x都包括n维特征 $x=\\left(x_1, x_2, \\cdots, x_n\\right)$，类标记集合含有k种类别，即 $y=\\left(y_1, y_2, \\cdots, y_n\\right)$ 此时的后验记为 P\\left(y_k \\mid x\\right)=\\frac{P\\left(x \\mid y_k\\right) \\times P\\left(y_k\\right)}{\\sum_k P\\left(x \\mid y_k\\right) \\times P\\left(y_k\\right)} 朴素贝叶斯算法对条件概率分布作出了独立性的假设，通俗地讲就是说假设各个维度的特征 $ {x_1},{x_2}, \\cdots ,{x_n} $ 互相独立， 所以分子分母的条件概率都能改写为 P\\left(x \\mid y_k\\right)=P\\left(x_1, x_2, \\cdots, x_n \\mid y_k\\right)=\\prod_{i=1}^n P\\left(x_i \\mid y_k\\right) 所以朴素贝叶斯可以改写为： f(x)=\\underset{y_k}{\\arg \\max } P\\left(y_k \\mid x\\right)=\\underset{y_k}{\\arg \\max } \\frac{P\\left(y_k\\right) \\times \\prod_{i=1}^n P\\left(x_i \\mid y_k\\right)}{\\sum_k P\\left(y_k\\right) \\times \\prod_{i=1}^n P\\left(x_i \\mid y_k\\right)} 因为对所有的 $ y_k $ ，上式中的分母的值都是一样的，所以可以忽略分母部分 优点 朴素贝叶斯模型有稳定的分类效率。 对小规模的数据表现很好，能处理多分类任务，适合增量式训练，尤其是数据量超出内存时，可以一批批的去增量训练。 对缺失数据不太敏感，算法也比较简单，常用于文本分类。 缺点 需要知道先验概率，且先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。 对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）。 蒙特卡洛 理论基础是大数定律。大数定律是描述相当多次数重复试验的结果的定律，在大数定理的保证下：利用事件发生的 频率 作为事件发生的 概率的近似值。 蒙特卡罗方法一般分为三个步骤，包括构造随机的概率的过程，从构造随机概率分布中抽样，求解估计量。 一般情况下，蒙特卡罗算法的特点是，采样越多，越近似最优解，而永远不是最优解。 优点：对于具有统计性质的问题可以直接进行解决，对于连续性的问题也不必进行离散化处理。 缺点：1、对于确定性问题转化成随机性问题做的估值处理，丧失精确性，得到一个接近准确的N值也不太容易。 2、如果解空间的可能情况很多则很难求解（NP问题）","link":"/2022/09/21/statistics/"}],"tags":[{"name":"ML","slug":"ML","link":"/tags/ML/"},{"name":"leetcode","slug":"leetcode","link":"/tags/leetcode/"},{"name":"algorithm","slug":"algorithm","link":"/tags/algorithm/"}],"categories":[],"pages":[]}