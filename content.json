{"posts":[{"title":"feature-engineering","text":"01 normalization 不做归一化的结果会倾向于数值差别比较大的特征 Q1 为什么需要对数值类型的特征做归一化? 将所有的特征都统一到一个大致相同的数值区间内。 包括线性函数归一化(Min-Max Scaling)，使 结果映射到[0, 1]的范围，实现对原始数据的等比缩放,() X_{\\text {norm }}=\\frac{X-X_{\\min }}{X_{\\max }-X_{\\min }} 标准化：零均值标准化(Z-Score Normalization) 它会将原始数据映射到均值为 0、标准差为1的分布上。具体来说，假设原始特征的均值为μ、标准差为σ，那么 归一化公式定义为 z=\\frac{x-\\mu}{\\sigma} 归一化的优点：e.g. 容易更快地通过梯度下降找到最优解。 数据归一化对需要使用梯度下降的模型起作用，对决策树模型不产生影响 两者比较： 计算距离中发挥相同的作用，应该选择标准化，标准化更适合现代嘈杂大数据场景 想保留原始数据中由标准差所反映的潜在权重关系，或数据不符合正态分布时，选择归一化。 RE Q2.逻辑回归必须要进行标准化吗？如果你不用正则，标准化并不是必须的，如果用正则，那么标准化是必须的。当然，无论哪种情况，标准化都有益无害，不用正则时，归一化是影响参数更新的快慢，但加上正则，则正则项偏向于关注数值范围大参数。 Q3.如何处理类别型特征有限选项内取值的特征，e.g. gender（male, female），blood types（AB， A，B）通常为字符串，通常要转为数值型（除决策树等少数model） 序号编码：有大小关系的， e.g. 高度，成绩，可直接转为1，2，3（保留了大小关系） one-hot ： 不具有大小关系的， 优点： 使用稀疏向量来节省空间（某一维取值为 1，其他位置取值均为0） 将离散特征扩展到欧式空间，离散特征就对应对应空间中的某个点 便于配合特征选择降维 二进制编码 且维数少于独热编码，节省了存储空间。 特征组合 把一阶离散特征两两组合，构成高阶组合特征。若组合后的矩阵 M x N 太大了，则进行矩阵分解将需要学习的参数的规模变为m×k+n×k Q4.特征组合寻找方法见决策树， Q5.文本模型 模型 词袋模型：就是将每篇文章看成一袋子 词，并忽略每个词出现的顺序。具体地说，就是将整段文本以词为单位切分开， 然后每篇文章可以表示成一个长向量，向量中的每一维代表一个单词，而该维对 应的权重则反映了这个词在原文章中的重要程度。常用TF-IDF来计算权重， TF-IDF \\operatorname{TF}-\\operatorname{IDF}(t, d)=\\mathrm{TF}(t, d) \\times \\operatorname{IDF}(t),其中TF(t,d)为单词t在文档d中出现的频率，IDF(t)是逆文档频率，用来衡量单词t对表达语义所起的重要性，表示为直观的解释是，如果一个单词在非常多的文章里面都出现，那么它可能是一个比较通用的词汇，对于区分某篇文章特殊语义的贡献较小，因此对权重做一定惩罚 n-gram可以将连续 出现的n个词(n≤N)组成的词组(N-gram)也作为一个单独的特征放到向量表示 中去，构成N-gram模型。同时还会对单词进行词干抽取(Word Stemming)处理，即将不 同词性的单词统一成为同一词干的形式。 Word2Vec一种浅层的神经网络模型，由CBOW(Continues Bag of Words)和Skip-gram组合。CBOW的目标是根据上下文出现的词语来预测当前词的生成概率;而Skip-gram是根据当前词来预测上下文中各词的生成概率。 LDALDA是利用文档中单词的共现关 系来对单词按主题聚类，也可以理解为对“文档-单词”矩阵进行分解，得到“文档- 主题”和“主题-单词”两个概率分布。而Word2Vec其实是对“上下文-单词”矩阵进行 学习，其中上下文由周围的几个单词组成，由此得到的词向量表示更多地融入了 上下文共现的特征。 图数据不足的处理方法清洗特征处理 时间类 间隔型：离某一特殊时间点的距离 离散型：是否属于某一特殊时间段 组合型 文本型 n-gram TF-IDF 预处理- scaling（便于收敛）","link":"/2022/09/28/feature-engineering/"},{"title":"welcome to my blog","text":"This is my first post. Check my GitHub for more info. If you get any problems you can ask me via my Email:se5ame@foxmail.com. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2022/09/21/hello-world/"},{"title":"leetcode-algorithm","text":"DP（dynamic programming） note:从较小规模的问题通过递归的方式来解决问题，通过定义：初始化状态和 状态转移方程 来求解 剑指 Offer 63. 股票的最大利润题目 1234567891011class Solution:def maxProfit(self, prices: List[int]) -&gt; int: max = 0 min = float(inf) for i in prices: min = min(i, min) max = (max, i - min) if max &lt; 0: return 0 return max 状态定义： 设动态规划列表 dpdpdp ，dp[i]dp[i]dp[i] 代表以 prices[i]prices[i]prices[i] 为结尾的子数组的最大利润 状态转移方程：买卖股票一次，所以比较前i-1天的最大profit和第i天的profit note: 比较可以用min()，max()代替if语句 斐波那契 和 青蛙跳台阶 双指针 对撞指针求list中的和为s的两个数 dfs深度优先往子节点最深处遍历，到最底层子节点后网上返回，返回过程中能够继续往子节点遍历没有经过的节点。 代码逻辑：递归 函数中先写最底层的判断（包括越界，数值相等） 在递归调用的过程设计过程函数 bfsBFS 通常借助 队列 的先入先出特性来实现，在元素出队的时候应把其子节点入队","link":"/2022/09/24/leetcode-algorithm/"},{"title":"pandas &amp; plot","text":"plot（visualization）sns.pairplot看数据相关性，两两比较 1234from sklearn.datasets import load_irisimport matplotlib.pyplot as pltimport seaborn as snsimport pandas as pd parameter data为画图使用的数据，hue 是数据的label 1sns.pairplot(data,hue='target') vars:只留几个特征两两比较 12sns.pairplot(data1,hue='target',vars=['sepal length (cm)', 'sepal width (cm)']); palette 调整颜色 123sns.pairplot(data1,hue='target',palette={'setosa':'b', 'versicolor':'deepskyblue', 'virginica':'#43C6C3'}); sns.heatmap（热力图）123456789101112# 使用seaborn画图而不是pltsns.set()# data:数据 square:是否是正方形 vmax:最大值 vmin:最小值 robust:排除极端值影响sns.heatmap(data=grid, square=True, vmax=20, vmin=0, robust=True)# 标题plt.title(&quot;test&quot;)# 保存图片plt.savefig(&quot;../data/grid.png&quot;)# 显示图片plt.show() 如果是DataFrame则以列名标记，而不是index","link":"/2022/09/27/pandas/"},{"title":"statistics","text":"概率论1.极大似然估计MLE利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值前提：满足独立同分布 对于这个函数：p(x \\mid \\theta) 其中 $ x $ 表示 具体数据 $ \\theta $ 表示模型参数如果 $ \\theta $ 是已知确定的，x是变量，这个函数叫做 概率函数(probability function)，它描述对于不同的样本点 ，其出现概率是多少。如果 $ x $ 是已知确定的，\\theta 是变量，这个函数叫做 似然函数(likelihood function) , 它描述对于不同的模型参数，出现 这个样本点的概率是多少。 e.g.1.有一个罐子，里面有黑白两种颜色的球，数目多少不知，两种颜色的比例也不知。想知道罐中白球和黑球的比例前面的一百次重复记录中，有七十次是白球，请问罐中白球所占的比例最有可能是多少? 解：如果第一次抽象的结果记为x1,第二次抽样的结果记为x2….那么样本结果为(x1,x2…..,x100)。这样，我们可以得到如下表达式：P(样本结果|Model) \\begin{aligned} &=P(x 1, x 2, \\ldots, x 100 \\mid \\text { Model }) \\\\ &=P(x 1 \\mid M e l) P(x 2 \\mid M) \\ldots P(x 100 \\mid M) \\\\ &=p^{\\wedge} 70(1-p)^{\\wedge} 30 . \\end{aligned}样本结果出现的可能性最大，也就是使得 p^70(1-p)^30 值最大，那么我们就可以看成是p的方程，求导即可！ 那么既然事情已经发生了，为什么不让这个出现的结果的可能性最大呢？这也就是最大似然估计的核心。通常将上式两边去对数，将乘法变为加法，结果不变，方便求导 2.贝叶斯和最大后验MAP 2.1贝叶斯公式 \\mathrm{P}(\\mathrm{A} \\mid \\mathrm{B})=\\frac{\\mathrm{P}(\\mathrm{B} \\mid \\mathrm{A}) \\mathrm{P}(\\mathrm{A})}{\\mathrm{P}(\\mathrm{B})} 将 $P(B)$ 用全概率公式展开（连续型变量用积分表离散型用求和表示） \\mathrm{P}(\\mathrm{A} \\mid \\mathrm{B})=\\frac{\\mathrm{P}(\\mathrm{B} \\mid \\mathrm{A}) \\mathrm{P}(\\mathrm{A})}{\\mathrm{P}(\\mathrm{B} \\mid \\mathrm{A}) \\mathrm{P}(\\mathrm{A})+\\mathrm{P}(\\mathrm{B} \\mid \\sim \\mathrm{A}) \\mathrm{P}(\\sim \\mathrm{A})} 其中 $ P(A) $ 为先验，为已知值。对于投硬币的例子来看，我们认为（”先验地知道“）$ \\theta $ 取0.5的概率很大，取其他值的概率小一些。可以用一个高斯分布来具体描述这个先验知识，例如假设 $P(\\theta)$ 为均值0.5，方差0.1的高斯函数，$ P(B \\mid A)$ 称似然函数$P(A \\mid B)$ 称为后验 最大后验估计MAP 其中分母为已知值（这是一个可以由数据集得到的值），$P(\\theta)$ 也已知，MAP于MLE的区别就是MAP多乘了一个先验 $P(\\theta)$ 朴素贝叶斯 给定训练数据集，其中每个样本x都包括n维特征 $x=\\left(x_1, x_2, \\cdots, x_n\\right)$，类标记集合含有k种类别，即 $y=\\left(y_1, y_2, \\cdots, y_n\\right)$ 此时的后验记为 P\\left(y_k \\mid x\\right)=\\frac{P\\left(x \\mid y_k\\right) \\times P\\left(y_k\\right)}{\\sum_k P\\left(x \\mid y_k\\right) \\times P\\left(y_k\\right)} 朴素贝叶斯算法对条件概率分布作出了独立性的假设，通俗地讲就是说假设各个维度的特征 $ {x_1},{x_2}, \\cdots ,{x_n} $ 互相独立， 所以分子分母的条件概率都能改写为 P\\left(x \\mid y_k\\right)=P\\left(x_1, x_2, \\cdots, x_n \\mid y_k\\right)=\\prod_{i=1}^n P\\left(x_i \\mid y_k\\right) 所以朴素贝叶斯可以改写为： f(x)=\\underset{y_k}{\\arg \\max } P\\left(y_k \\mid x\\right)=\\underset{y_k}{\\arg \\max } \\frac{P\\left(y_k\\right) \\times \\prod_{i=1}^n P\\left(x_i \\mid y_k\\right)}{\\sum_k P\\left(y_k\\right) \\times \\prod_{i=1}^n P\\left(x_i \\mid y_k\\right)} 因为对所有的 $ y_k $ ，上式中的分母的值都是一样的，所以可以忽略分母部分 优点 朴素贝叶斯模型有稳定的分类效率。 对小规模的数据表现很好，能处理多分类任务，适合增量式训练，尤其是数据量超出内存时，可以一批批的去增量训练。 对缺失数据不太敏感，算法也比较简单，常用于文本分类。 缺点 需要知道先验概率，且先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。 对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）。 蒙特卡洛 理论基础是大数定律。大数定律是描述相当多次数重复试验的结果的定律，在大数定理的保证下：利用事件发生的 频率 作为事件发生的 概率的近似值。 蒙特卡罗方法一般分为三个步骤，包括构造随机的概率的过程，从构造随机概率分布中抽样，求解估计量。 一般情况下，蒙特卡罗算法的特点是，采样越多，越近似最优解，而永远不是最优解。 优点：对于具有统计性质的问题可以直接进行解决，对于连续性的问题也不必进行离散化处理。 缺点：1、对于确定性问题转化成随机性问题做的估值处理，丧失精确性，得到一个接近准确的N值也不太容易。 2、如果解空间的可能情况很多则很难求解（NP问题）","link":"/2022/09/21/statistics/"},{"title":"model-estimation","text":"模型评估Accuracy的局限性 当样本不平衡时acc不能作为评估标准 精确率与召回率的权衡精确率Precision：‘分类正确的正样本个数占分类器判定为正样本的样本’个数的比例。召回率recall：’分类正确的正样本个数占真正的正样本个数’的比例。 Precision值和Recall值是既矛盾又统一的两个指标，为了提高Precision值，分 类器需要尽量在“更有把握”时才把样本预测为正样本，但此时往往会因为过于保 守而漏掉很多“没有把握”的正样本，导致Recall值降低。 F1 score 是P-R的综合反映 \\mathrm{F} 1=\\frac{2 \\times \\text { precisio } \\times \\text { recall }}{\\text { precision }+\\text { recall }} .RMSE经常被用来衡量回归模型的好坏 conclusion ：只从单一的评估指标出发去评估模型，往往会得出片面甚至错误的结论，需要通过一组互补的指标去评估模 型，才能更好地发现并解决模型存在的问题，从而更好地解决实际业务场景中遇到的问题。 ROCROC曲线经常作为评估二值分类器最重要的指标之一ROC曲线的横坐标为假阳性率(False Positive Rate，FPR);纵坐标为真阳性率(True Positive Rate，TPR) F P R=\\frac{F P}{N} T P R=\\frac{T P}{P}P是真实的正样本的数量，N是真实的负样本的数量，TP是P个正样本中 被分类器预测为正样本的个数，FP是N个负样本中被分类器预测为正样本的个数。 ROC曲线能够尽量降低不同测试集带来的干扰，更加客观地衡量模型本身的性能,稳定地反映模型本身的好坏.选择P-R曲线还是ROC曲线是因实际问题而异的，如果研究者希望更多地看到模型在特定数据集上的表现，P-R曲线则能够更直观地反映其性能。 AUC计算AUC值只需要沿着ROC横轴做积分就可以了。 由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值一般在0.5~1之 间。AUC越大，说明分类器越可能把真正的正样本排在前面，分类性能越好。 余弦相似度\\cos (A, B)=\\frac{A \\cdot B}{\\|A\\|_2\\|B\\|_2}即两个向量 夹角的余弦，关注的是向量之间的角度关系，并不关心它们的绝对大小，其取值 范围是[−1,1]。当一对文本相似度的长度差距很大、但内容相近时，如果使用词频 或词向量作为特征，它们在特征空间中的的欧氏距离通常很大;而如果使用余弦 相似度的话，它们之间的夹角可能很小，因而相似度高。研究的对象的特征维度往往很高，余弦相似度在高维情况下依然保 持“相同时为1，正交时为0，相反时为−1”的性质，而欧氏距离的数值则受维度的 影响，范围不固定，并且含义也比较模糊。 Word2Vec中，其向量的模长是经过归一化的，此时欧氏距 离与余弦距离有着单调的关系，即\\|A-B\\|_2=\\sqrt{2(1-\\cos (A, B))}如果选择距离最小(相似度最大)的近邻，那么使用余弦相似度和欧氏距离的结果是相同的。不同情况采取的方法不同，欧氏距离体现数值上的绝对差异，而余弦距离体现方向上的相对差异。统计两部剧的用户观看行为，用户A的观看向量为(0,1)，用户B为 (1,0);此时二者的余弦距离很大，而欧氏距离很小;我们分析两个用户对于不同 视频的偏好，更关注相对差异，显然应当使用余弦距离。而当我们分析用户活跃 度，以登陆次数(单位:次)和平均观看时长(单位:分钟)作为特征时，余弦距离会 认为(1,10)、(10,100)两个用户距离很近;但显然这两个用户活跃度是有着极大差异 的，此时我们更关注数值绝对差异，应当使用欧氏距离。 余弦距离是否是一个严格定义的距离? 集合中，如果每一对元素均可唯一确定一个实数，使得三条距离公理(正定性，对称性，三角不等式)成立 \\operatorname{dist}(A, B)=1-\\cos \\theta=\\frac{\\|A\\|_2\\|B\\|_2-A B}{\\|A\\|_2\\|B\\|_2} . 分子恒大于0，满足正定 对称性 dist（A， B）=dist（B， A） 三角不等式 dist（A， B）+ dist（B， c）有可能小于 dist（a， c） 同时在单位圆上余弦距离与欧式距离的平方成正比，$\\operatorname{dist}(A, B)=\\frac{1}{2}|A-B|^2$，所以不是严格定义的距离3 A/B测试是验证模型最终效果的主要手段。","link":"/2022/10/04/model-estimation/"},{"title":"CPP","text":"data structure线性表List1234void unionL(List *La, list Lb) int La_len, Lb_len, i La_len = ListLength(*La) Lb_len = ListLength(Lb) 删除操作通用 主函数写在public中，调用函数在private中，private的函数中的变量要重新传，不能用public中的变量 fun中的参数定义时要加属性(bool res(int i)) 没有py中 if not 的形式 用新变量要要给属性 123456789101112ElemType data # ElemType 表示任意类型元素# length() 只能获取字符串长度 如:string str = &quot;ADAS&quot;;int len = str.length();# size() 可以获取vector和字符串的长度 ,从1开始计数#交换两个值swap(nums[i], nums[j]) vector向量（Vector）是一个封装了动态大小数组的顺序容器（Sequence Container）。向量是一个能够存放任意类型的动态数组。‘vector a(5)’; //定义了5个整型元素的向量（&lt;&gt;中为元素类型名，它可以是任何合法的数据类型），但没有给出初值，其值是不确定的.‘vector a(5,1)’;//定义了5个整型元素的向量,且给出每个元素的初值为1‘vector a(b)’; //用b向量来创建a向量，整体复制性赋值 1234567891011121314151617181920# init vectorvector&lt;int&gt;nums; //不指定长度vector&lt;int&gt;nums(n); // 指定长度为n# addnums.push_back(1);//直接从数组末端添加nums[i] = 1;//直接赋值给第i个位置# delnums.resize(nums.size-i); //直接将数组长度减小，某种方式上删掉了后面i个nums.pop_back();//删掉最后一个元素# 遍历for(int i = 0; i &lt; nums.size(); i++) cout&lt;&lt;nums[i] &lt;&lt;endl; # 翻转reverse(nums.begin(), nums.end());","link":"/2022/10/13/CPP/"},{"title":"DL","text":"CNN receptive field： typical setting: all channels kernel size = 3 x 3 stride = 2(移动步长) 通常会overlap，超出范围做padding(可以取平均或者用边缘值补充) parameter sharing pooling 类似于激活函数，缩小图片的操作，有：max pooling，mean pooling等，目的是减少运算量subsampling","link":"/2022/10/05/DL/"},{"title":"hadoop","text":"hadoop分布式系统架构，解决数据的存储和计算1.xx和2.xx的区别 yarnresourceManager：集群资源的总体NodeManager：管理单节点的资源 Linux 指令大全1234567mkdir runoob # 当前目录下创建文件夹# 当前目录下的 runoob2 目录中，建立一个名为 test 的子目录。若runoob2 目录原本不存在，则建立一个。（注：本例若不加 -p 参数，且原本 runoob2 目录不存在，则产生错误。）mkdir -p runoob2/test","link":"/2022/10/17/hadoop/"}],"tags":[{"name":"ML","slug":"ML","link":"/tags/ML/"},{"name":"leetcode","slug":"leetcode","link":"/tags/leetcode/"},{"name":"algorithm","slug":"algorithm","link":"/tags/algorithm/"}],"categories":[],"pages":[]}